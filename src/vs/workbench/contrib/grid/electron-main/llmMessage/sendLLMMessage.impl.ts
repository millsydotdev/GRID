/*---------------------------------------------------------------------------------------------
 *  Copyright (c) 2025 Millsy.dev. All rights reserved.
 *  Licensed under the Apache License, Version 2.0. See LICENSE.txt for more information.
 *--------------------------------------------------------------------------------------------*/

// disable foreign import complaints
/* eslint-disable */
import Anthropic from '@anthropic-ai/sdk';
import { Ollama } from 'ollama';
import OpenAI, { ClientOptions, AzureOpenAI } from 'openai';
import { MistralCore } from '@mistralai/mistralai/core.js';
import { fimComplete } from '@mistralai/mistralai/funcs/fimComplete.js';
import { Tool as GeminiTool, FunctionDeclaration, GoogleGenAI, ThinkingConfig, Schema, Type } from '@google/genai';
import { GoogleAuth } from 'google-auth-library';
/* eslint-enable */

import {
	GeminiLLMChatMessage,
	LLMChatMessage,
	LLMFIMMessage,
	ModelListParams,
	OllamaModelResponse,
	OnError,
	OnFinalMessage,
	OnText,
	RawToolCallObj,
	RawToolParamsObj,
} from '../../common/sendLLMMessageTypes.js';
import {
	ChatMode,
	displayInfoOfProviderName,
	FeatureName,
	ModelSelectionOptions,
	OverridesOfModel,
	ProviderName,
	SettingsOfProvider,
} from '../../common/gridSettingsTypes.js';
import {
	getSendableReasoningInfo,
	getModelCapabilities,
	getProviderCapabilities,
	defaultProviderSettings,
	getReservedOutputTokenSpace,
} from '../../common/modelCapabilities.js';
import { extractReasoningWrapper, extractXMLToolsWrapper } from './extractGrammar.js';
import { availableTools, InternalToolInfo } from '../../common/prompt/prompts.js';
import { generateUuid } from '../../../../../base/common/uuid.js';
import { PROVIDER_REGISTRY } from '../../common/providerRegistry.js';

const getGoogleApiKey = async () => {
	// module‑level singleton
	const auth = new GoogleAuth({ scopes: `https://www.googleapis.com/auth/cloud-platform` });
	const key = await auth.getAccessToken();
	if (!key) { throw new Error(`Google API failed to generate a key.`); }
	return key;
};

type InternalCommonMessageParams = {
	onText: OnText;
	onFinalMessage: OnFinalMessage;
	onError: OnError;
	providerName: ProviderName;
	settingsOfProvider: SettingsOfProvider;
	modelSelectionOptions: ModelSelectionOptions | undefined;
	overridesOfModel: OverridesOfModel | undefined;
	modelName: string;
	_setAborter: (aborter: () => void) => void;
};

type SendChatParams_Internal = InternalCommonMessageParams & {
	messages: LLMChatMessage[];
	separateSystemMessage: string | undefined;
	chatMode: ChatMode | null;
	mcpTools: InternalToolInfo[] | undefined;
};
type SendFIMParams_Internal = InternalCommonMessageParams & {
	messages: LLMFIMMessage;
	separateSystemMessage: string | undefined;
	featureName?: FeatureName;
};
export type ListParams_Internal<ModelResponse> = ModelListParams<ModelResponse>;

const invalidApiKeyMessage = (providerName: ProviderName) =>
	`Invalid ${displayInfoOfProviderName(providerName).title} API key.`;

// ------------ SDK POOLING FOR LOCAL PROVIDERS ------------

/**
 * In-memory cache for OpenAI-compatible SDK clients (for local providers only).
 * Keyed by: `${providerName}:${endpoint}:${apiKeyHash}`
 * This avoids recreating clients on every request, improving connection reuse.
 */
const openAIClientCache = new Map<string, OpenAI>();

/**
 * In-memory cache for Ollama SDK clients.
 * Keyed by: `${endpoint}`
 */
const ollamaClientCache = new Map<string, Ollama>();

/**
 * Simple hash function for API keys (for cache key generation).
 * Only used for local providers where security is less critical.
 */
const hashApiKey = (apiKey: string | undefined): string => {
	if (!apiKey) { return 'noop'; }
	// Simple hash - just use first 8 chars for cache key (not for security)
	return apiKey.substring(0, 8);
};

/**
 * Build cache key for OpenAI-compatible client.
 * Format: `${providerName}:${endpoint}:${apiKeyHash}`
 */
const buildOpenAICacheKey = (providerName: ProviderName, settingsOfProvider: SettingsOfProvider): string => {
	let endpoint = '';
	let apiKey = 'noop';

	if (providerName === 'openAI') {
		apiKey = settingsOfProvider[providerName]?.apiKey || '';
	} else if (providerName === 'ollama' || providerName === 'vLLM' || providerName === 'lmStudio') {
		endpoint = settingsOfProvider[providerName]?.endpoint || '';
	} else if (providerName === 'openAICompatible' || providerName === 'liteLLM') {
		endpoint = settingsOfProvider[providerName]?.endpoint || '';
		apiKey = settingsOfProvider[providerName]?.apiKey || '';
	}

	return `${providerName}:${endpoint}:${hashApiKey(apiKey)}`;
};

/**
 * Get or create OpenAI-compatible client with caching for local providers.
 * For local providers (ollama, vLLM, lmStudio, localhost openAICompatible/liteLLM),
 * we cache clients to reuse connections. Cloud providers always get new instances.
 */
const getOpenAICompatibleClient = async ({
	settingsOfProvider,
	providerName,
	includeInPayload,
}: {
	settingsOfProvider: SettingsOfProvider;
	providerName: ProviderName;
	includeInPayload?: Record<string, unknown>;
}): Promise<OpenAI> => {
	// Detect if this is a local provider
	const isExplicitLocalProvider = providerName === 'ollama' || providerName === 'vLLM' || providerName === 'lmStudio';
	let isLocalhostEndpoint = false;
	if (providerName === 'openAICompatible' || providerName === 'liteLLM') {
		const endpoint = settingsOfProvider[providerName]?.endpoint || '';
		if (endpoint) {
			try {
				const url = new URL(endpoint);
				const hostname = url.hostname.toLowerCase();
				isLocalhostEndpoint =
					hostname === 'localhost' || hostname === '127.0.0.1' || hostname === '0.0.0.0' || hostname === '::1';
			} catch (e) {
				isLocalhostEndpoint = false;
			}
		}
	}
	const isLocalProvider = isExplicitLocalProvider || isLocalhostEndpoint;

	// Only cache for local providers
	if (isLocalProvider) {
		const cacheKey = buildOpenAICacheKey(providerName, settingsOfProvider);
		const cached = openAIClientCache.get(cacheKey);
		if (cached) {
			return cached;
		}
	}

	// Create new client (will cache if local)
	const client = await newOpenAICompatibleSDK({ settingsOfProvider, providerName, includeInPayload });

	// Cache if local provider
	if (isLocalProvider) {
		const cacheKey = buildOpenAICacheKey(providerName, settingsOfProvider);
		openAIClientCache.set(cacheKey, client);
	}

	return client;
};

/**
 * Get or create Ollama client with caching.
 */
const getOllamaClient = ({ endpoint }: { endpoint: string }): Ollama => {
	if (!endpoint) {
		throw new Error(
			`Ollama Endpoint was empty (please enter ${defaultProviderSettings.ollama.endpoint} in GRID Settings if you want the default url).`
		);
	}

	const cached = ollamaClientCache.get(endpoint);
	if (cached) {
		return cached;
	}

	const ollama = new Ollama({ host: endpoint });
	ollamaClientCache.set(endpoint, ollama);
	return ollama;
};

// ------------ OPENAI-COMPATIBLE (HELPERS) ------------

const parseHeadersJSON = (s: string | undefined): Record<string, string | null | undefined> | undefined => {
	if (!s) { return undefined; }
	try {
		return JSON.parse(s);
	} catch (e) {
		throw new Error(`Error parsing OpenAI-Compatible headers: ${s} is not a valid JSON.`);
	}
};

/**
 * Compute max_tokens/num_predict for local providers based on feature.
 * For local models, we use smaller token limits to reduce latency:
 * - Autocomplete: 64-96 tokens (very small, fast completions)
 * - Ctrl+K / Apply: 150-250 tokens (small edits)
 * - Other/Cloud: 300 tokens (default)
 */
const computeMaxTokensForLocalProvider = (isLocalProvider: boolean, featureName: FeatureName | undefined): number => {
	if (!isLocalProvider) {
		return 300; // Default for cloud providers
	}

	// Infer feature from featureName or default to safe value
	if (featureName === 'Autocomplete') {
		return 96; // Small value for fast autocomplete
	} else if (featureName === 'Ctrl+K' || featureName === 'Apply') {
		return 200; // Medium value for quick edits
	}

	// Default for local providers when featureName is unknown
	return 300;
};

const newOpenAICompatibleSDK = async ({
	settingsOfProvider,
	providerName,
	includeInPayload,
}: {
	settingsOfProvider: SettingsOfProvider;
	providerName: ProviderName;
	includeInPayload?: Record<string, unknown>;
}) => {
	// Network optimizations: timeouts and connection reuse
	// The OpenAI SDK handles HTTP keep-alive and connection pooling internally
	// Use shorter timeout for local models (they're on localhost, should be fast)

	// Detect local providers: explicit local providers + localhost endpoints
	const isExplicitLocalProvider = providerName === 'ollama' || providerName === 'vLLM' || providerName === 'lmStudio';
	let isLocalhostEndpoint = false;
	if (providerName === 'openAICompatible' || providerName === 'liteLLM') {
		const endpoint = settingsOfProvider[providerName]?.endpoint || '';
		if (endpoint) {
			try {
				// Use proper URL parsing to check hostname (not substring matching)
				const url = new URL(endpoint);
				const hostname = url.hostname.toLowerCase();
				isLocalhostEndpoint =
					hostname === 'localhost' || hostname === '127.0.0.1' || hostname === '0.0.0.0' || hostname === '::1';
			} catch (e) {
				// Invalid URL - assume non-local (safe default)
				isLocalhostEndpoint = false;
			}
		}
	}
	const isLocalProvider = isExplicitLocalProvider || isLocalhostEndpoint;

	const timeoutMs = isLocalProvider ? 30_000 : 60_000; // 30s for local, 60s for remote
	const commonPayloadOpts: ClientOptions = {
		dangerouslyAllowBrowser: true,
		timeout: timeoutMs,
		maxRetries: 1, // Reduce retries for local models (they fail fast if not available)
		// Enable HTTP/2 and connection reuse for better performance
		// For localhost, connection reuse is especially important to avoid TCP handshake overhead
		// The OpenAI SDK uses keep-alive by default, which is optimal for localhost
		httpAgent: undefined, // Let SDK handle connection pooling (optimized for localhost)
		...includeInPayload,
	};
	if (providerName === 'openAI') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else if (providerName === 'ollama') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: `${thisConfig.endpoint}/v1`, apiKey: 'noop', ...commonPayloadOpts });
	} else if (providerName === 'vLLM') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: `${thisConfig.endpoint}/v1`, apiKey: 'noop', ...commonPayloadOpts });
	} else if (providerName === 'liteLLM') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: `${thisConfig.endpoint}/v1`, apiKey: 'noop', ...commonPayloadOpts });
	} else if (providerName === 'lmStudio') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: `${thisConfig.endpoint}/v1`, apiKey: 'noop', ...commonPayloadOpts });
	} else if (providerName === 'openRouter') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({
			baseURL: 'https://openrouter.ai/api/v1',
			apiKey: thisConfig.apiKey,
			defaultHeaders: {
				'HTTP-Referer': 'https://grid.com', // Optional, for including your app on openrouter.ai rankings.
				'X-Title': 'GRID', // Optional. Shows in rankings on openrouter.ai.
			},
			...commonPayloadOpts,
		});
	} else if (providerName === 'googleVertex') {
		// https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library
		const thisConfig = settingsOfProvider[providerName];
		const baseURL = `https://${thisConfig.region}-aiplatform.googleapis.com/v1/projects/${thisConfig.project}/locations/${thisConfig.region}/endpoints/${'openapi'}`;
		const apiKey = await getGoogleApiKey();
		return new OpenAI({ baseURL: baseURL, apiKey: apiKey, ...commonPayloadOpts });
	} else if (providerName === 'microsoftAzure') {
		// https://learn.microsoft.com/en-us/rest/api/aifoundry/model-inference/get-chat-completions/get-chat-completions?view=rest-aifoundry-model-inference-2024-05-01-preview&tabs=HTTP
		//  https://github.com/openai/openai-node?tab=readme-ov-file#microsoft-azure-openai
		const thisConfig = settingsOfProvider[providerName];
		const endpoint = `https://${thisConfig.project}.openai.azure.com/`;
		const apiVersion = thisConfig.azureApiVersion ?? '2024-04-01-preview';
		const options = { endpoint, apiKey: thisConfig.apiKey, apiVersion };
		return new AzureOpenAI({ ...options, ...commonPayloadOpts });
	} else if (providerName === 'awsBedrock') {
		/**
		 * We treat Bedrock as *OpenAI-compatible only through a proxy*:
		 *   • LiteLLM default → http://localhost:4000/v1
		 *   • Bedrock-Access-Gateway → https://<api-id>.execute-api.<region>.amazonaws.com/openai/
		 *
		 * The native Bedrock runtime endpoint
		 *   https://bedrock-runtime.<region>.amazonaws.com
		 * is **NOT** OpenAI-compatible, so we do *not* fall back to it here.
		 */
		const { endpoint, apiKey } = settingsOfProvider.awsBedrock;

		// ① use the user-supplied proxy if present
		// ② otherwise default to local LiteLLM
		let baseURL = endpoint || 'http://localhost:4000/v1';

		// Normalize: make sure we end with “/v1”
		if (!baseURL.endsWith('/v1')) { baseURL = baseURL.replace(/\/+$/, '') + '/v1'; }

		return new OpenAI({ baseURL, apiKey, ...commonPayloadOpts });
	} else if (providerName === 'deepseek') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: 'https://api.deepseek.com/v1', apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else if (providerName === 'openAICompatible') {
		const thisConfig = settingsOfProvider[providerName];
		const headers = parseHeadersJSON(thisConfig.headersJSON);
		return new OpenAI({
			baseURL: thisConfig.endpoint,
			apiKey: thisConfig.apiKey,
			defaultHeaders: headers,
			...commonPayloadOpts,
		});
	} else if (providerName === 'groq') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: 'https://api.groq.com/openai/v1', apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else if (providerName === 'xAI') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: 'https://api.x.ai/v1', apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else if (providerName === 'mistral') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: 'https://api.mistral.ai/v1', apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else if (providerName === 'huggingFace') {
		const thisConfig = settingsOfProvider[providerName];
		return new OpenAI({ baseURL: 'https://router.huggingface.co/v1', apiKey: thisConfig.apiKey, ...commonPayloadOpts });
	} else {
		// Fallback: Check registry for generic OpenAI compatible providers
		// This supports the 50+ providers defined in PROVIDER_REGISTRY without explicit headers here
		const registryEntry = PROVIDER_REGISTRY.find((p) => p.id === providerName);
		if (registryEntry && registryEntry.endpoint) {
			const thisConfig = settingsOfProvider[providerName];
			// If user specifies endpoint in settings (overriding registry), use it.
			// Casting to any because not all settings types explicitly declare 'endpoint',
			// but we want to support it if present in the config object at runtime.
			const configEndpoint = (thisConfig as { endpoint?: string })?.endpoint;
			const baseURL = configEndpoint || registryEntry.endpoint;

			return new OpenAI({ baseURL, apiKey: thisConfig.apiKey, ...commonPayloadOpts });
		}

		throw new Error(`GRID providerName was invalid: ${providerName}.`);
	}
};

const _sendOpenAICompatibleFIM = async ({
	messages: { prefix, suffix, stopTokens },
	onFinalMessage,
	onError,
	settingsOfProvider,
	modelName: modelName_,
	_setAborter,
	providerName,
	overridesOfModel,
	onText,
	featureName,
}: SendFIMParams_Internal) => {
	const { modelName, supportsFIM, additionalOpenAIPayload } = getModelCapabilities(
		providerName,
		modelName_,
		overridesOfModel
	);

	if (!supportsFIM) {
		if (modelName === modelName_) { onError({ message: `Model ${modelName} does not support FIM.`, fullError: null }); }
		else { onError({ message: `Model ${modelName_} (${modelName}) does not support FIM.`, fullError: null }); }
		return;
	}

	// Detect if this is a local provider for streaming optimization
	const isExplicitLocalProvider = providerName === 'ollama' || providerName === 'vLLM' || providerName === 'lmStudio';
	let isLocalhostEndpoint = false;
	if (providerName === 'openAICompatible' || providerName === 'liteLLM') {
		const endpoint = settingsOfProvider[providerName]?.endpoint || '';
		if (endpoint) {
			try {
				// Use proper URL parsing to check hostname (not substring matching)
				const url = new URL(endpoint);
				const hostname = url.hostname.toLowerCase();
				isLocalhostEndpoint =
					hostname === 'localhost' || hostname === '127.0.0.1' || hostname === '0.0.0.0' || hostname === '::1';
			} catch (e) {
				// Invalid URL - assume non-local (safe default)
				isLocalhostEndpoint = false;
			}
		}
	}
	const isLocalProvider = isExplicitLocalProvider || isLocalhostEndpoint;

	const openai = await getOpenAICompatibleClient({
		providerName,
		settingsOfProvider,
		includeInPayload: additionalOpenAIPayload,
	});

	// Compute max_tokens based on feature and provider type
	const maxTokensForThisCall = computeMaxTokensForLocalProvider(isLocalProvider, featureName);

	// For local models, use streaming FIM for better responsiveness
	// Only stream if onText is provided and not empty (some consumers like autocomplete have empty onText)
	if (isLocalProvider && onText && typeof onText === 'function') {
		let fullText = '';
		let firstTokenReceived = false;
		const firstTokenTimeout = 10_000; // 10 seconds for first token on local models

		const stream = await openai.completions.create({
			model: modelName,
			prompt: prefix,
			suffix: suffix,
			stop: stopTokens,
			max_tokens: maxTokensForThisCall,
			stream: true,
		});

		_setAborter(() => stream.controller?.abort());

		// Set up first token timeout for local models
		const firstTokenTimeoutId = setTimeout(() => {
			if (!firstTokenReceived) {
				stream.controller?.abort();
				onError({
					message: 'Local model took too long to respond for autocomplete. Try a smaller model or a cloud model.',
					fullError: null,
				});
			}
		}, firstTokenTimeout);

		try {
			for await (const chunk of stream) {
				// Mark first token received
				if (!firstTokenReceived) {
					firstTokenReceived = true;
					clearTimeout(firstTokenTimeoutId);
				}

				const newText = chunk.choices[0]?.text ?? '';
				fullText += newText;
				onText({
					fullText,
					fullReasoning: '',
					toolCall: undefined,
				});
			}

			// Clear timeout on successful completion
			clearTimeout(firstTokenTimeoutId);
			onFinalMessage({ fullText, fullReasoning: '', anthropicReasoning: null });
		} catch (streamError) {
			clearTimeout(firstTokenTimeoutId);
			onError({
				message: streamError + '',
				fullError: streamError instanceof Error ? streamError : new Error(String(streamError)),
			});
		}
	} else {
		// Non-streaming for remote models (fallback)
		openai.completions
			.create({
				model: modelName,
				prompt: prefix,
				suffix: suffix,
				stop: stopTokens,
				max_tokens: maxTokensForThisCall,
			})
			.then(async (response) => {
				const fullText = response.choices[0]?.text;
				onFinalMessage({ fullText, fullReasoning: '', anthropicReasoning: null });
			})
			.catch((error) => {
				if (error instanceof OpenAI.APIError && error.status === 401) {
					onError({ message: invalidApiKeyMessage(providerName), fullError: error });
				} else {
					onError({ message: error + '', fullError: error });
				}
			});
	}
};

const toOpenAICompatibleTool = (toolInfo: InternalToolInfo) => {
	const { name, description, params } = toolInfo;

	const paramsWithType: { [s: string]: { description: string; type: 'string' } } = {};
	for (const key in params) {
		paramsWithType[key] = { ...params[key], type: 'string' };
	}

	return {
		type: 'function',
		function: {
			name: name,
			// strict: true, // strict mode - https://platform.openai.com/docs/guides/function-calling?api-mode=chat
			description: description,
			parameters: {
				type: 'object',
				properties: params,
				// required: Object.keys(params), // in strict mode, all params are required and additionalProperties is false
				// additionalProperties: false,
			},
		},
	} satisfies OpenAI.Chat.Completions.ChatCompletionTool;
};

const openAITools = (chatMode: ChatMode | null, mcpTools: InternalToolInfo[] | undefined) => {
	const allowedTools = availableTools(chatMode, mcpTools);
	if (!allowedTools || Object.keys(allowedTools).length === 0) { return null; }

	const openAITools: OpenAI.Chat.Completions.ChatCompletionTool[] = [];
	for (const t in allowedTools ?? {}) {
		openAITools.push(toOpenAICompatibleTool(allowedTools[t]));
	}
	return openAITools;
};

// convert LLM tool call to our tool format
const rawToolCallObjOfParamsStr = (name: string, toolParamsStr: string, id: string): RawToolCallObj | null => {
	let input: unknown;
	try {
		input = JSON.parse(toolParamsStr);
	} catch (e) {
		return null;
	}

	if (input === null) { return null; }
	if (typeof input !== 'object') { return null; }

	const rawParams: RawToolParamsObj = input as RawToolParamsObj;
	return { id, name, rawParams, doneParams: Object.keys(rawParams), isDone: true };
};

const rawToolCallObjOfAnthropicParams = (toolBlock: Anthropic.Messages.ToolUseBlock): RawToolCallObj | null => {
	const { id, name, input } = toolBlock;

	if (input === null) { return null; }
	if (typeof input !== 'object') { return null; }

	const rawParams: RawToolParamsObj = input;
	return { id, name, rawParams, doneParams: Object.keys(rawParams), isDone: true };
};

// ------------ OPENAI-COMPATIBLE ------------

const _sendOpenAICompatibleChat = async ({
	messages,
	onText,
	onFinalMessage,
	onError,
	settingsOfProvider,
	modelSelectionOptions,
	modelName: modelName_,
	_setAborter,
	providerName,
	chatMode,
	separateSystemMessage,
	overridesOfModel,
	mcpTools,
}: SendChatParams_Internal) => {
	const { modelName, specialToolFormat, reasoningCapabilities, additionalOpenAIPayload } = getModelCapabilities(
		providerName,
		modelName_,
		overridesOfModel
	);

	const { providerReasoningIOSettings } = getProviderCapabilities(providerName);

	// reasoning
	const { canIOReasoning, openSourceThinkTags } = reasoningCapabilities || {};
	const reasoningInfo = getSendableReasoningInfo(
		'Chat',
		providerName,
		modelName_,
		modelSelectionOptions,
		overridesOfModel
	); // user's modelName_ here

	const includeInPayload = {
		...providerReasoningIOSettings?.input?.includeInPayload?.(reasoningInfo),
		...additionalOpenAIPayload,
	};

	// tools
	const potentialTools = openAITools(chatMode, mcpTools);
	const nativeToolsObj =
		potentialTools && specialToolFormat === 'openai-style' ? ({ tools: potentialTools } as const) : {};

	// instance
	const openai: OpenAI = await getOpenAICompatibleClient({ providerName, settingsOfProvider, includeInPayload });
	if (providerName === 'microsoftAzure') {
		// Required to select the model
		(openai as AzureOpenAI).deploymentName = modelName;
	}

	// open source models - manually parse think tokens
	const { needsManualParse: needsManualReasoningParse, nameOfFieldInDelta: nameOfReasoningFieldInDelta } =
		providerReasoningIOSettings?.output ?? {};
	const manuallyParseReasoning = needsManualReasoningParse && canIOReasoning && openSourceThinkTags;
	if (manuallyParseReasoning) {
		const { newOnText, newOnFinalMessage } = extractReasoningWrapper(onText, onFinalMessage, openSourceThinkTags);
		onText = newOnText;
		onFinalMessage = newOnFinalMessage;
	}

	// manually parse out tool results if XML
	if (!specialToolFormat) {
		const { newOnText, newOnFinalMessage } = extractXMLToolsWrapper(onText, onFinalMessage, chatMode, mcpTools);
		onText = newOnText;
		onFinalMessage = newOnFinalMessage;
	}

	// Variables for tracking response state
	let fullReasoningSoFar = '';
	let fullTextSoFar = '';
	let toolName = '';
	let toolId = '';
	let toolParamsStr = '';
	let isRetrying = false; // Flag to prevent processing streaming chunks during retry

	// Detect if this is a local provider for timeout optimization
	const isExplicitLocalProviderChat =
		providerName === 'ollama' || providerName === 'vLLM' || providerName === 'lmStudio';
	let isLocalhostEndpointChat = false;
	if (providerName === 'openAICompatible' || providerName === 'liteLLM') {
		const endpoint = settingsOfProvider[providerName]?.endpoint || '';
		if (endpoint) {
			try {
				const url = new URL(endpoint);
				const hostname = url.hostname.toLowerCase();
				isLocalhostEndpointChat =
					hostname === 'localhost' || hostname === '127.0.0.1' || hostname === '0.0.0.0' || hostname === '::1';
			} catch (e) {
				isLocalhostEndpointChat = false;
			}
		}
	}
	const isLocalChat = isExplicitLocalProviderChat || isLocalhostEndpointChat;

	// Helper function to process streaming response
	const processStreamingResponse = async (response: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk> & { controller: AbortController }) => {
		_setAborter(() => response.controller.abort());

		// For local models, add hard timeout with partial results
		const overallTimeout = isLocalChat ? 20_000 : 120_000; // 20s for local, 120s for remote
		const firstTokenTimeout = isLocalChat ? 10_000 : 30_000; // 10s for first token on local

		let firstTokenReceived = false;

		// Set up overall timeout
		const timeoutId = setTimeout(() => {
			if (fullTextSoFar || fullReasoningSoFar || toolName) {
				// We have partial results - commit them
				const toolCall = rawToolCallObjOfParamsStr(toolName, toolParamsStr, toolId);
				const toolCallObj = toolCall ? { toolCall } : {};
				onFinalMessage({
					fullText: fullTextSoFar,
					fullReasoning: fullReasoningSoFar,
					anthropicReasoning: null,
					...toolCallObj,
				});
				// Note: We don't call onError here since we have partial results
			} else {
				// No tokens received - abort
				response.controller?.abort();
				onError({
					message: isLocalChat
						? 'Local model timed out. Try a smaller model or use a cloud model for this task.'
						: 'Request timed out.',
					fullError: null,
				});
			}
		}, overallTimeout);

		// Set up first token timeout (only for local models)
		let firstTokenTimeoutId: ReturnType<typeof setTimeout> | null = null;
		if (isLocalChat) {
			firstTokenTimeoutId = setTimeout(() => {
				if (!firstTokenReceived) {
					response.controller?.abort();
					onError({
						message:
							'Local model is too slow (no response after 10s). Try a smaller/faster model or use a cloud model.',
						fullError: null,
					});
				}
			}, firstTokenTimeout);
		}

		try {
			// when receive text
			for await (const chunk of response) {
				// Check if we're retrying (another response is being processed)
				if (isRetrying) {
					clearTimeout(timeoutId);
					if (firstTokenTimeoutId) { clearTimeout(firstTokenTimeoutId); }
					return; // Stop processing this streaming response, retry is in progress
				}

				// Mark first token received
				if (!firstTokenReceived) {
					firstTokenReceived = true;
					if (firstTokenTimeoutId) {
						clearTimeout(firstTokenTimeoutId);
						firstTokenTimeoutId = null;
					}
				}

				// message
				const newText = chunk.choices[0]?.delta?.content ?? '';
				fullTextSoFar += newText;

				// tool call
				for (const tool of chunk.choices[0]?.delta?.tool_calls ?? []) {
					const index = tool.index;
					if (index !== 0) { continue; }

					toolName += tool.function?.name ?? '';
					toolParamsStr += tool.function?.arguments ?? '';
					toolId += tool.id ?? '';
				}

				// reasoning
				let newReasoning = '';
				if (nameOfReasoningFieldInDelta) {
					newReasoning = ((chunk.choices[0]?.delta as unknown as Record<string, unknown>)?.[nameOfReasoningFieldInDelta] || '') + '';
					fullReasoningSoFar += newReasoning;
				}

				// call onText
				onText({
					fullText: fullTextSoFar,
					fullReasoning: fullReasoningSoFar,
					toolCall: !toolName
						? undefined
						: { name: toolName, rawParams: {}, isDone: false, doneParams: [], id: toolId },
				});
			}

			// Clear timeouts on successful completion
			clearTimeout(timeoutId);
			if (firstTokenTimeoutId) { clearTimeout(firstTokenTimeoutId); }

			// on final
			if (!fullTextSoFar && !fullReasoningSoFar && !toolName) {
				onError({ message: 'GRID: Response from model was empty.', fullError: null });
			} else {
				const toolCall = rawToolCallObjOfParamsStr(toolName, toolParamsStr, toolId);
				const toolCallObj = toolCall ? { toolCall } : {};
				onFinalMessage({
					fullText: fullTextSoFar,
					fullReasoning: fullReasoningSoFar,
					anthropicReasoning: null,
					...toolCallObj,
				});
			}
		} catch (streamError) {
			clearTimeout(timeoutId);
			if (firstTokenTimeoutId) { clearTimeout(firstTokenTimeoutId); }
			// If error occurs during streaming, re-throw to be caught by outer catch handler
			throw streamError;
		}
	};

	// Helper function to process non-streaming response
	const processNonStreamingResponse = async (response: any) => {
		const choice = response.choices[0];
		if (!choice) {
			onError({ message: 'GRID: Response from model was empty.', fullError: null });
			return;
		}

		const fullText = choice.message?.content ?? '';
		const toolCalls = choice.message?.tool_calls ?? [];

		if (toolCalls.length > 0) {
			const toolCall = toolCalls[0];
			toolName = toolCall.function?.name ?? '';
			toolParamsStr = toolCall.function?.arguments ?? '';
			toolId = toolCall.id ?? '';
		}

		// Call onText once with full text
		onText({
			fullText: fullText,
			fullReasoning: '',
			toolCall: !toolName ? undefined : { name: toolName, rawParams: {}, isDone: false, doneParams: [], id: toolId },
		});

		// Call onFinalMessage
		const toolCall = rawToolCallObjOfParamsStr(toolName, toolParamsStr, toolId);
		const toolCallObj = toolCall ? { toolCall } : {};
		onFinalMessage({ fullText: fullText, fullReasoning: '', anthropicReasoning: null, ...toolCallObj });
	};

	// Try streaming first
	const options: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {
		model: modelName,
		messages: messages as any,
		stream: true,
		...nativeToolsObj,
		...additionalOpenAIPayload,
		// max_completion_tokens: maxTokens,
	};

	// Flag to ensure we only process one response (prevent duplicate processing)
	// Use object reference to ensure atomic updates across async operations
	const processingState = { responseProcessed: false, isProcessing: false };
	let streamingResponse: any = null;

	openai.chat.completions
		.create(options)
		.then(async (response) => {
			// Atomic check-and-set to prevent race conditions
			if (processingState.responseProcessed || processingState.isProcessing || isRetrying) {
				return; // Guard against duplicate processing
			}
			processingState.isProcessing = true;
			streamingResponse = response;
			try {
				await processStreamingResponse(response);
				processingState.responseProcessed = true;
			} finally {
				processingState.isProcessing = false;
			}
		})
		// when error/fail - this catches errors of both .create() and .then(for await)
		.catch(async (error) => {
			// Abort streaming response if it's still running
			if (streamingResponse) {
				try {
					streamingResponse.controller?.abort();
				} catch (e) {
					// Ignore abort errors
				}
			}

			// Check if this is the organization verification error for streaming
			if (
				error instanceof OpenAI.APIError &&
				error.status === 400 &&
				error.code === 'unsupported_value' &&
				error.param === 'stream' &&
				error.message?.includes('organization must be verified')
			) {
				// Set retry flag to stop processing any remaining streaming chunks
				isRetrying = true;

				// Reset state variables before retrying to prevent duplicate content
				fullTextSoFar = '';
				fullReasoningSoFar = '';
				toolName = '';
				toolId = '';
				toolParamsStr = '';

				// Retry with streaming disabled (only retry the API call, not the entire message flow)
				// Silently retry - don't show error notification for organization verification issues
				const nonStreamingOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsNonStreaming = {
					model: modelName,
					messages: messages as any,
					stream: false,
					...nativeToolsObj,
					...additionalOpenAIPayload,
				};

				try {
					const response = await openai.chat.completions.create(nonStreamingOptions);
					// Atomic check-and-set to prevent race conditions
					if (processingState.responseProcessed || processingState.isProcessing || !isRetrying) {
						return; // Guard against duplicate processing
					}
					processingState.isProcessing = true;
					try {
						await processNonStreamingResponse(response);
						processingState.responseProcessed = true;
					} finally {
						processingState.isProcessing = false;
					}
					isRetrying = false;
					// Successfully retried with non-streaming - silently continue, no error notification
					return; // Exit early to prevent showing any error
				} catch (retryError) {
					// Log the retry failure for debugging (but don't show confusing error to user)
					console.debug(
						'[sendLLMMessage] Retry with non-streaming also failed:',
						retryError instanceof Error ? retryError.message : String(retryError)
					);
					// If retry also fails, show a generic error instead of silently failing
					// This prevents users from wondering why the model isn't responding
					onError({
						message: 'Failed to get response from model. Please check your API key and organization settings.',
						fullError: retryError instanceof Error ? retryError : new Error(String(retryError)),
					});
					return;
				}
			}
			// Check if this is a "model does not support tools" error (e.g., from Ollama)
			else if (
				error instanceof OpenAI.APIError &&
				error.status === 400 &&
				(error.message?.toLowerCase().includes('does not support tools') ||
					(error.message?.toLowerCase().includes('tool') && error.message?.toLowerCase().includes('not support')))
			) {
				// Set retry flag to stop processing any remaining streaming chunks
				isRetrying = true;

				// Reset state variables before retrying to prevent duplicate content
				fullTextSoFar = '';
				fullReasoningSoFar = '';
				toolName = '';
				toolId = '';
				toolParamsStr = '';

				// Retry without tools - this model doesn't support native tool calling
				// Fall back to XML-based tool calling or regular chat
				// CRITICAL: Retry immediately without delay for tool support errors (they're fast to detect)
				const optionsWithoutTools: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {
					model: modelName,
					messages: messages as any,
					stream: true,
					// Explicitly omit tools - don't include nativeToolsObj
					...additionalOpenAIPayload,
				};

				try {
					// Use same timeout as original request (already optimized for local models)
					const response = await openai.chat.completions.create(optionsWithoutTools);
					// Atomic check-and-set to prevent race conditions
					if (processingState.responseProcessed || processingState.isProcessing || !isRetrying) {
						return; // Guard against duplicate processing
					}
					processingState.isProcessing = true;
					streamingResponse = response;
					try {
						await processStreamingResponse(response);
						processingState.responseProcessed = true;
					} finally {
						processingState.isProcessing = false;
					}
					isRetrying = false;
					// Successfully retried without tools - silently continue
					// Note: XML-based tool calling will still work if the model supports it
					return; // Exit early to prevent showing any error
				} catch (retryError) {
					// Log the retry failure for debugging
					console.debug(
						'[sendLLMMessage] Retry without tools also failed:',
						retryError instanceof Error ? retryError.message : String(retryError)
					);
					// If retry also fails, show the original error
					onError({
						message: `Model does not support tool calling: ${error.message || 'Unknown error'}`,
						fullError: retryError instanceof Error ? retryError : new Error(String(retryError)),
					});
					return;
				}
			} else if (error instanceof OpenAI.APIError && error.status === 401) {
				onError({ message: invalidApiKeyMessage(providerName), fullError: error });
			} else if (error instanceof OpenAI.APIError && error.status === 429) {
				// Rate limit exceeded - don't retry immediately, show clear error
				const rateLimitMessage = error.message || 'Rate limit exceeded. Please wait a moment before trying again.';
				onError({ message: `Rate limit exceeded: ${rateLimitMessage}`, fullError: error });
			} else {
				onError({ message: error + '', fullError: error });
			}
		});
};

type OpenAIModel = {
	id: string;
	created: number;
	object: 'model';
	owned_by: string;
};
const _openaiCompatibleList = async ({
	onSuccess: onSuccess_,
	onError: onError_,
	settingsOfProvider,
	providerName,
}: ListParams_Internal<OpenAIModel>) => {
	const onSuccess = ({ models }: { models: OpenAIModel[] }) => {
		onSuccess_({ models });
	};
	const onError = ({ error }: { error: string }) => {
		onError_({ error });
	};
	try {
		const openai = await getOpenAICompatibleClient({ providerName, settingsOfProvider });
		openai.models
			.list()
			.then(async (response) => {
				const models: OpenAIModel[] = [];
				models.push(...response.data);
				while (response.hasNextPage()) {
					models.push(...(await response.getNextPage()).data);
				}
				onSuccess({ models });
			})
			.catch((error) => {
				onError({ error: error + '' });
			});
	} catch (error) {
		onError({ error: error + '' });
	}
};

// ------------ ANTHROPIC (HELPERS) ------------
const toAnthropicTool = (toolInfo: InternalToolInfo) => {
	const { name, description, params } = toolInfo;
	const paramsWithType: { [s: string]: { description: string; type: 'string' } } = {};
	for (const key in params) {
		paramsWithType[key] = { ...params[key], type: 'string' };
	}
	return {
		name: name,
		description: description,
		input_schema: {
			type: 'object',
			properties: paramsWithType,
			// required: Object.keys(params),
		},
	} satisfies Anthropic.Messages.Tool;
};

const anthropicTools = (chatMode: ChatMode | null, mcpTools: InternalToolInfo[] | undefined) => {
	const allowedTools = availableTools(chatMode, mcpTools);
	if (!allowedTools || Object.keys(allowedTools).length === 0) { return null; }

	const anthropicTools: Anthropic.Messages.ToolUnion[] = [];
	for (const t in allowedTools ?? {}) {
		anthropicTools.push(toAnthropicTool(allowedTools[t]));
	}
	return anthropicTools;
};

// ------------ ANTHROPIC ------------
const sendAnthropicChat = async ({
	messages,
	providerName,
	onText,
	onFinalMessage,
	onError,
	settingsOfProvider,
	modelSelectionOptions,
	overridesOfModel,
	modelName: modelName_,
	_setAborter,
	separateSystemMessage,
	chatMode,
	mcpTools,
}: SendChatParams_Internal) => {
	const { modelName, specialToolFormat } = getModelCapabilities(providerName, modelName_, overridesOfModel);

	const thisConfig = settingsOfProvider.anthropic;
	const { providerReasoningIOSettings } = getProviderCapabilities(providerName);

	// reasoning
	const reasoningInfo = getSendableReasoningInfo(
		'Chat',
		providerName,
		modelName_,
		modelSelectionOptions,
		overridesOfModel
	); // user's modelName_ here
	const includeInPayload = providerReasoningIOSettings?.input?.includeInPayload?.(reasoningInfo) || {};

	// anthropic-specific - max tokens
	const maxTokens = getReservedOutputTokenSpace(providerName, modelName_, {
		isReasoningEnabled: !!reasoningInfo?.isReasoningEnabled,
		overridesOfModel,
	});

	// tools
	const potentialTools = anthropicTools(chatMode, mcpTools);
	const nativeToolsObj =
		potentialTools && specialToolFormat === 'anthropic-style'
			? ({ tools: potentialTools, tool_choice: { type: 'auto' } } as const)
			: {};

	// instance
	const anthropic = new Anthropic({
		apiKey: thisConfig.apiKey,
		dangerouslyAllowBrowser: true,
		timeout: 60_000, // 60s timeout
		maxRetries: 2, // Fast retries for transient errors
		// Connection reuse is handled internally by the SDK
	});

	const stream = anthropic.messages.stream({
		system: separateSystemMessage ?? undefined,
		messages: messages as any, // AnthropicLLMChatMessage type may not exactly match SDK's MessageParam, but is compatible at runtime
		model: modelName,
		max_tokens: maxTokens ?? 4_096, // anthropic requires this
		...includeInPayload,
		...nativeToolsObj,
	});

	// manually parse out tool results if XML
	if (!specialToolFormat) {
		const { newOnText, newOnFinalMessage } = extractXMLToolsWrapper(onText, onFinalMessage, chatMode, mcpTools);
		onText = newOnText;
		onFinalMessage = newOnFinalMessage;
	}

	// when receive text
	let fullText = '';
	let fullReasoning = '';

	let fullToolName = '';
	let fullToolParams = '';

	const runOnText = () => {
		onText({
			fullText,
			fullReasoning,
			// Note: Temporary ID during streaming; real ID will be provided in finalMessage
			toolCall: !fullToolName
				? undefined
				: { name: fullToolName, rawParams: {}, isDone: false, doneParams: [], id: 'streaming_temp' },
		});
	};
	// there are no events for tool_use, it comes in at the end
	stream.on('streamEvent', (e) => {
		// start block
		if (e.type === 'content_block_start') {
			if (e.content_block.type === 'text') {
				if (fullText) { fullText += '\n\n'; } // starting a 2nd text block
				fullText += e.content_block.text;
				runOnText();
			} else if (e.content_block.type === 'thinking') {
				if (fullReasoning) { fullReasoning += '\n\n'; } // starting a 2nd reasoning block
				fullReasoning += e.content_block.thinking;
				runOnText();
			} else if (e.content_block.type === 'redacted_thinking') {
				console.log('delta', e.content_block.type);
				if (fullReasoning) { fullReasoning += '\n\n'; } // starting a 2nd reasoning block
				fullReasoning += '[redacted_thinking]';
				runOnText();
			} else if (e.content_block.type === 'tool_use') {
				fullToolName += e.content_block.name ?? ''; // anthropic gives us the tool name in the start block
				runOnText();
			}
		}

		// delta
		else if (e.type === 'content_block_delta') {
			if (e.delta.type === 'text_delta') {
				fullText += e.delta.text;
				runOnText();
			} else if (e.delta.type === 'thinking_delta') {
				fullReasoning += e.delta.thinking;
				runOnText();
			} else if (e.delta.type === 'input_json_delta') {
				// tool use
				fullToolParams += e.delta.partial_json ?? ''; // anthropic gives us the partial delta (string) here - https://docs.anthropic.com/en/api/messages-streaming
				runOnText();
			}
		}
	});

	// on done - (or when error/fail) - this is called AFTER last streamEvent
	stream.on('finalMessage', (response) => {
		const anthropicReasoning = response.content.filter((c) => c.type === 'thinking' || c.type === 'redacted_thinking');
		const tools = response.content.filter((c) => c.type === 'tool_use');
		// console.log('TOOLS!!!!!!', JSON.stringify(tools, null, 2))
		// console.log('TOOLS!!!!!!', JSON.stringify(response, null, 2))
		const toolCall = tools[0] && rawToolCallObjOfAnthropicParams(tools[0]);
		const toolCallObj = toolCall ? { toolCall } : {};

		onFinalMessage({ fullText, fullReasoning, anthropicReasoning, ...toolCallObj });
	});
	// on error
	stream.on('error', (error) => {
		if (error instanceof Anthropic.APIError && error.status === 401) {
			onError({ message: invalidApiKeyMessage(providerName), fullError: error });
		} else {
			onError({ message: error + '', fullError: error });
		}
	});
	_setAborter(() => stream.controller.abort());
};

// ------------ MISTRAL ------------
// https://docs.mistral.ai/api/#tag/fim
const sendMistralFIM = ({
	messages,
	onFinalMessage,
	onError,
	settingsOfProvider,
	overridesOfModel,
	modelName: modelName_,
	_setAborter,
	providerName,
}: SendFIMParams_Internal) => {
	const { modelName, supportsFIM } = getModelCapabilities(providerName, modelName_, overridesOfModel);
	if (!supportsFIM) {
		if (modelName === modelName_) { onError({ message: `Model ${modelName} does not support FIM.`, fullError: null }); }
		else { onError({ message: `Model ${modelName_} (${modelName}) does not support FIM.`, fullError: null }); }
		return;
	}

	const mistral = new MistralCore({ apiKey: settingsOfProvider.mistral.apiKey });
	fimComplete(mistral, {
		model: modelName,
		prompt: messages.prefix,
		suffix: messages.suffix,
		stream: false,
		maxTokens: 300,
		stop: messages.stopTokens,
	})
		.then(async (response) => {
			// unfortunately, _setAborter() does not exist
			const content = response?.ok ? (response.value.choices?.[0]?.message?.content ?? '') : '';
			const fullText =
				typeof content === 'string'
					? content
					: content.map((chunk) => (chunk.type === 'text' ? chunk.text : '')).join('');

			onFinalMessage({ fullText, fullReasoning: '', anthropicReasoning: null });
		})
		.catch((error) => {
			onError({ message: error + '', fullError: error });
		});
};

// ------------ OLLAMA ------------

const ollamaList = async ({
	onSuccess: onSuccess_,
	onError: onError_,
	settingsOfProvider,
}: ListParams_Internal<OllamaModelResponse>) => {
	const onSuccess = ({ models }: { models: OllamaModelResponse[] }) => {
		onSuccess_({ models });
	};
	const onError = ({ error }: { error: string }) => {
		onError_({ error });
	};
	try {
		const thisConfig = settingsOfProvider.ollama;
		const ollama = getOllamaClient({ endpoint: thisConfig.endpoint });
		ollama
			.list()
			.then((response) => {
				const { models } = response;
				onSuccess({ models });
			})
			.catch((error) => {
				onError({ error: error + '' });
			});
	} catch (error) {
		onError({ error: error + '' });
	}
};

const sendOllamaFIM = ({
	messages,
	onFinalMessage,
	onError,
	settingsOfProvider,
	modelName,
	_setAborter,
	featureName,
	onText,
}: SendFIMParams_Internal) => {
	const thisConfig = settingsOfProvider.ollama;
	const ollama = getOllamaClient({ endpoint: thisConfig.endpoint });

	// Compute num_predict based on feature (Ollama is always local)
	const numPredictForThisCall = computeMaxTokensForLocalProvider(true, featureName);

	let fullText = '';
	ollama
		.generate({
			model: modelName,
			prompt: messages.prefix,
			suffix: messages.suffix,
			options: {
				stop: messages.stopTokens,
				num_predict: numPredictForThisCall,
				// repeat_penalty: 1,
			},
			raw: true,
			stream: true, // stream is not necessary but lets us expose the
		})
		.then(async (stream) => {
			_setAborter(() => stream.abort());
			for await (const chunk of stream) {
				const newText = chunk.response;
				fullText += newText;
				// Call onText during streaming for incremental UI updates (like OpenAI-compatible FIM)
				// This enables true streaming UX for Ollama autocomplete
				if (onText && typeof onText === 'function') {
					onText({
						fullText,
						fullReasoning: '',
						toolCall: undefined,
					});
				}
			}
			onFinalMessage({ fullText, fullReasoning: '', anthropicReasoning: null });
		})
		// when error/fail
		.catch((error) => {
			onError({ message: error + '', fullError: error });
		});
};

// ---------------- GEMINI NATIVE IMPLEMENTATION ----------------

const toGeminiFunctionDecl = (toolInfo: InternalToolInfo) => {
	const { name, description, params } = toolInfo;
	return {
		name,
		description,
		parameters: {
			type: Type.OBJECT,
			properties: Object.entries(params).reduce(
				(acc, [key, value]) => {
					acc[key] = {
						type: Type.STRING,
						description: value.description,
					};
					return acc;
				},
				{} as Record<string, Schema>
			),
		},
	} satisfies FunctionDeclaration;
};

const geminiTools = (chatMode: ChatMode | null, mcpTools: InternalToolInfo[] | undefined): GeminiTool[] | null => {
	const allowedTools = availableTools(chatMode, mcpTools);
	if (!allowedTools || Object.keys(allowedTools).length === 0) { return null; }
	const functionDecls: FunctionDeclaration[] = [];
	for (const t in allowedTools ?? {}) {
		functionDecls.push(toGeminiFunctionDecl(allowedTools[t]));
	}
	const tools: GeminiTool = { functionDeclarations: functionDecls };
	return [tools];
};

// Implementation for Gemini using Google's native API
const sendGeminiChat = async ({
	messages,
	separateSystemMessage,
	onText,
	onFinalMessage,
	onError,
	settingsOfProvider,
	overridesOfModel,
	modelName: modelName_,
	_setAborter,
	providerName,
	modelSelectionOptions,
	chatMode,
	mcpTools,
}: SendChatParams_Internal) => {
	if (providerName !== 'gemini') { throw new Error(`Sending Gemini chat, but provider was ${providerName}`); }

	const thisConfig = settingsOfProvider[providerName];

	const {
		modelName,
		specialToolFormat,
		// reasoningCapabilities,
	} = getModelCapabilities(providerName, modelName_, overridesOfModel);

	// const { providerReasoningIOSettings } = getProviderCapabilities(providerName)

	// reasoning
	// const { canIOReasoning, openSourceThinkTags, } = reasoningCapabilities || {}
	const reasoningInfo = getSendableReasoningInfo(
		'Chat',
		providerName,
		modelName_,
		modelSelectionOptions,
		overridesOfModel
	); // user's modelName_ here
	// const includeInPayload = providerReasoningIOSettings?.input?.includeInPayload?.(reasoningInfo) || {}

	const thinkingConfig: ThinkingConfig | undefined = !reasoningInfo?.isReasoningEnabled
		? undefined
		: reasoningInfo.type === 'budget_slider_value'
			? { thinkingBudget: reasoningInfo.reasoningBudget }
			: undefined;

	// tools
	const potentialTools = geminiTools(chatMode, mcpTools);
	const toolConfig = potentialTools && specialToolFormat === 'gemini-style' ? potentialTools : undefined;

	// instance
	const genAI = new GoogleGenAI({ apiKey: thisConfig.apiKey });

	// manually parse out tool results if XML
	if (!specialToolFormat) {
		const { newOnText, newOnFinalMessage } = extractXMLToolsWrapper(onText, onFinalMessage, chatMode, mcpTools);
		onText = newOnText;
		onFinalMessage = newOnFinalMessage;
	}

	// when receive text
	const fullReasoningSoFar = '';
	let fullTextSoFar = '';

	let toolName = '';
	let toolParamsStr = '';
	let toolId = '';

	genAI.models
		.generateContentStream({
			model: modelName,
			config: {
				systemInstruction: separateSystemMessage,
				thinkingConfig: thinkingConfig,
				tools: toolConfig,
			},
			contents: messages as GeminiLLMChatMessage[],
		})
		.then(async (stream) => {
			_setAborter(() => {
				stream.return(fullTextSoFar);
			});

			// Process the stream
			for await (const chunk of stream) {
				// message
				const newText = chunk.text ?? '';
				fullTextSoFar += newText;

				// tool call
				const functionCalls = chunk.functionCalls;
				if (functionCalls && functionCalls.length > 0) {
					const functionCall = functionCalls[0]; // Get the first function call
					toolName = functionCall.name ?? '';
					toolParamsStr = JSON.stringify(functionCall.args ?? {});
					toolId = functionCall.id ?? '';
				}

				// (do not handle reasoning yet)

				// call onText
				onText({
					fullText: fullTextSoFar,
					fullReasoning: fullReasoningSoFar,
					toolCall: !toolName
						? undefined
						: { name: toolName, rawParams: {}, isDone: false, doneParams: [], id: toolId },
				});
			}

			// on final
			if (!fullTextSoFar && !fullReasoningSoFar && !toolName) {
				onError({ message: 'GRID: Response from model was empty.', fullError: null });
			} else {
				if (!toolId) { toolId = generateUuid(); } // ids are empty, but other providers might expect an id
				const toolCall = rawToolCallObjOfParamsStr(toolName, toolParamsStr, toolId);
				const toolCallObj = toolCall ? { toolCall } : {};
				onFinalMessage({
					fullText: fullTextSoFar,
					fullReasoning: fullReasoningSoFar,
					anthropicReasoning: null,
					...toolCallObj,
				});
			}
		})
		.catch((error) => {
			const message = error?.message;
			if (typeof message === 'string') {
				if (error.message?.includes('API key')) {
					onError({ message: invalidApiKeyMessage(providerName), fullError: error });
				} else if (
					error?.message?.includes('429') ||
					error?.message?.includes('RESOURCE_EXHAUSTED') ||
					error?.message?.includes('quota')
				) {
					// Parse Gemini rate limit error to extract user-friendly message
					let rateLimitMessage = 'Rate limit reached. Please check your plan and billing details.';
					let retryDelay: string | undefined;

					try {
						// Try to parse the error message which may contain JSON
						let errorData: any = null;

						// First, try to parse the error message as JSON (it might be a JSON string)
						try {
							errorData = JSON.parse(error.message);
						} catch {
							// If that fails, check if error.message contains a JSON string
							const jsonMatch = error.message.match(/\{[\s\S]*\}/);
							if (jsonMatch) {
								errorData = JSON.parse(jsonMatch[0]);
							}
						}

						// Extract user-friendly message from nested structure
						if (errorData?.error?.message) {
							// The message might itself be a JSON string
							try {
								const innerError = JSON.parse(errorData.error.message);
								if (innerError?.error?.message) {
									rateLimitMessage = innerError.error.message;
									// Extract retry delay if available
									const retryInfo = innerError.error.details?.find(
										(d: any) => d['@type'] === 'type.googleapis.com/google.rpc.RetryInfo'
									);
									if (retryInfo?.retryDelay) {
										retryDelay = retryInfo.retryDelay;
									}
								}
							} catch {
								// If inner parse fails, use the outer message
								rateLimitMessage = errorData.error.message;
							}
						} else if (errorData?.error?.code === 429 || errorData?.error?.status === 'RESOURCE_EXHAUSTED') {
							// Fallback: use a generic rate limit message
							rateLimitMessage = 'You exceeded your current quota. Please check your plan and billing details.';
						}

						// Format the final message
						let finalMessage = rateLimitMessage;
						if (retryDelay) {
							// Parse retry delay (format: "57s" or "57.627694635s")
							const delaySeconds = parseFloat(retryDelay.replace('s', ''));
							const delayMinutes = Math.floor(delaySeconds / 60);
							const remainingSeconds = Math.ceil(delaySeconds % 60);
							if (delayMinutes > 0) {
								finalMessage += ` Please retry in ${delayMinutes} minute${delayMinutes > 1 ? 's' : ''}${remainingSeconds > 0 ? ` and ${remainingSeconds} second${remainingSeconds > 1 ? 's' : ''}` : ''}.`;
							} else {
								finalMessage += ` Please retry in ${Math.ceil(delaySeconds)} second${Math.ceil(delaySeconds) > 1 ? 's' : ''}.`;
							}
						} else {
							finalMessage += ' Please wait a moment before trying again.';
						}

						// Add helpful links
						finalMessage += ' For more information, see https://ai.google.dev/gemini-api/docs/rate-limits';

						onError({ message: finalMessage, fullError: error });
					} catch (parseError) {
						// If parsing fails, use a generic message
						onError({
							message:
								'Rate limit reached. Please check your Gemini API quota and billing details. See https://ai.google.dev/gemini-api/docs/rate-limits',
							fullError: error,
						});
					}
				} else { onError({ message: error + '', fullError: error }); }
			} else {
				onError({ message: error + '', fullError: error });
			}
		});
};

type CallFnOfProvider = {
	[providerName in ProviderName]: {
		sendChat: (params: SendChatParams_Internal) => Promise<void>;
		sendFIM: ((params: SendFIMParams_Internal) => void) | null;
		list: ((params: ListParams_Internal<any>) => void) | null;
	};
};

export const sendLLMMessageToProviderImplementation = {
	anthropic: {
		sendChat: sendAnthropicChat,
		sendFIM: null,
		list: null,
	},
	openAI: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	xAI: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	gemini: {
		sendChat: (params) => sendGeminiChat(params),
		sendFIM: null,
		list: null,
	},
	mistral: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: (params) => sendMistralFIM(params),
		list: null,
	},
	huggingFace: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	ollama: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: sendOllamaFIM,
		list: ollamaList,
	},
	openAICompatible: {
		sendChat: (params) => _sendOpenAICompatibleChat(params), // using openai's SDK is not ideal (your implementation might not do tools, reasoning, FIM etc correctly), talk to us for a custom integration
		sendFIM: (params) => _sendOpenAICompatibleFIM(params),
		list: null,
	},
	openRouter: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: (params) => _sendOpenAICompatibleFIM(params),
		list: null,
	},
	vLLM: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: (params) => _sendOpenAICompatibleFIM(params),
		list: (params) => _openaiCompatibleList(params),
	},
	deepseek: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	groq: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},

	lmStudio: {
		// lmStudio has no suffix parameter in /completions, so sendFIM might not work
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: (params) => _sendOpenAICompatibleFIM(params),
		list: (params) => _openaiCompatibleList(params),
	},
	liteLLM: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: (params) => _sendOpenAICompatibleFIM(params),
		list: null,
	},
	googleVertex: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	microsoftAzure: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
	awsBedrock: {
		sendChat: (params) => _sendOpenAICompatibleChat(params),
		sendFIM: null,
		list: null,
	},
} satisfies CallFnOfProvider;

/*
FIM info (this may be useful in the future with vLLM, but in most cases the only way to use FIM is if the provider explicitly supports it):

qwen2.5-coder https://ollama.com/library/qwen2.5-coder/blobs/e94a8ecb9327
<|fim_prefix|>{{ .Prompt }}<|fim_suffix|>{{ .Suffix }}<|fim_middle|>

codestral https://ollama.com/library/codestral/blobs/51707752a87c
[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}

deepseek-coder-v2 https://ollama.com/library/deepseek-coder-v2/blobs/22091531faf0
<｜fim▁begin｜>{{ .Prompt }}<｜fim▁hole｜>{{ .Suffix }}<｜fim▁end｜>

starcoder2 https://ollama.com/library/starcoder2/blobs/3b190e68fefe
<file_sep>
<fim_prefix>
{{ .Prompt }}<fim_suffix>{{ .Suffix }}<fim_middle>
<|end_of_text|>

codegemma https://ollama.com/library/codegemma:2b/blobs/48d9a8140749
<|fim_prefix|>{{ .Prompt }}<|fim_suffix|>{{ .Suffix }}<|fim_middle|>

*/
