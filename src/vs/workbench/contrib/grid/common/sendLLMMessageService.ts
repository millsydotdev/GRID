/*---------------------------------------------------------------------------------------------
 *  Copyright (c) 2025 Millsy.dev. All rights reserved.
 *  Licensed under the Apache License, Version 2.0. See LICENSE.txt for more information.
 *--------------------------------------------------------------------------------------------*/

import {
	EventLLMMessageOnTextParams,
	EventLLMMessageOnErrorParams,
	EventLLMMessageOnFinalMessageParams,
	ServiceSendLLMMessageParams,
	MainSendLLMMessageParams,
	MainLLMMessageAbortParams,
	ServiceModelListParams,
	EventModelListOnSuccessParams,
	EventModelListOnErrorParams,
	MainModelListParams,
	OllamaModelResponse,
	OpenaiCompatibleModelResponse,
} from './sendLLMMessageTypes.js';

import { createDecorator } from '../../../../platform/instantiation/common/instantiation.js';
import { registerSingleton, InstantiationType } from '../../../../platform/instantiation/common/extensions.js';
import { IChannel } from '../../../../base/parts/ipc/common/ipc.js';
import { IMainProcessService } from '../../../../platform/ipc/common/mainProcessService.js';
import { generateUuid } from '../../../../base/common/uuid.js';
import { Event } from '../../../../base/common/event.js';
import { Disposable } from '../../../../base/common/lifecycle.js';
import { IGridSettingsService } from './gridSettingsService.js';
import { IMCPService } from './mcpService.js';
import { ISecretDetectionService } from './secretDetectionService.js';
import { INotificationService } from '../../../../platform/notification/common/notification.js';

// calls channel to implement features
export const ILLMMessageService = createDecorator<ILLMMessageService>('llmMessageService');

export interface ILLMMessageService {
	readonly _serviceBrand: undefined;
	sendLLMMessage: (params: ServiceSendLLMMessageParams) => string | null;
	abort: (requestId: string) => void;
	ollamaList: (params: ServiceModelListParams<OllamaModelResponse>) => void;
	openAICompatibleList: (params: ServiceModelListParams<OpenaiCompatibleModelResponse>) => void;
}

// open this file side by side with llmMessageChannel
export class LLMMessageService extends Disposable implements ILLMMessageService {
	readonly _serviceBrand: undefined;
	private readonly channel: IChannel; // LLMMessageChannel

	// sendLLMMessage
	private readonly llmMessageHooks = {
		onText: {} as { [eventId: string]: (params: EventLLMMessageOnTextParams) => void },
		onFinalMessage: {} as { [eventId: string]: (params: EventLLMMessageOnFinalMessageParams) => void },
		onError: {} as { [eventId: string]: (params: EventLLMMessageOnErrorParams) => void },
		onAbort: {} as { [eventId: string]: () => void }, // NOT sent over the channel, result is instant when we call .abort()
	};

	// list hooks
	private readonly listHooks = {
		ollama: {
			success: {} as { [eventId: string]: (params: EventModelListOnSuccessParams<OllamaModelResponse>) => void },
			error: {} as { [eventId: string]: (params: EventModelListOnErrorParams<OllamaModelResponse>) => void },
		},
		openAICompat: {
			success: {} as {
				[eventId: string]: (params: EventModelListOnSuccessParams<OpenaiCompatibleModelResponse>) => void;
			},
			error: {} as { [eventId: string]: (params: EventModelListOnErrorParams<OpenaiCompatibleModelResponse>) => void },
		},
	} satisfies {
		[providerName in 'ollama' | 'openAICompat']: {
			success: { [eventId: string]: (params: EventModelListOnSuccessParams<any>) => void };
			error: { [eventId: string]: (params: EventModelListOnErrorParams<any>) => void };
		};
	};

	constructor(
		@IMainProcessService private readonly mainProcessService: IMainProcessService, // used as a renderer (only usable on client side)
		@IGridSettingsService private readonly gridSettingsService: IGridSettingsService,
		@INotificationService private readonly notificationService: INotificationService,
		@IMCPService private readonly mcpService: IMCPService,
		@ISecretDetectionService private readonly secretDetectionService: ISecretDetectionService
	) {
		super();

		// const service = ProxyChannel.toService<LLMMessageChannel>(mainProcessService.getChannel('grid-channel-sendLLMMessage')); // lets you call it like a service
		// see llmMessageChannel.ts
		this.channel = this.mainProcessService.getChannel('grid-channel-llmMessage');

		// .listen sets up an IPC channel and takes a few ms, so we set up listeners immediately and add hooks to them instead
		// llm
		this._register(
			(this.channel.listen('onText_sendLLMMessage') satisfies Event<EventLLMMessageOnTextParams>)((e) => {
				this.llmMessageHooks.onText[e.requestId]?.(e);
			})
		);
		this._register(
			(this.channel.listen('onFinalMessage_sendLLMMessage') satisfies Event<EventLLMMessageOnFinalMessageParams>)(
				(e) => {
					this.llmMessageHooks.onFinalMessage[e.requestId]?.(e);
					this._clearChannelHooks(e.requestId);
				}
			)
		);
		this._register(
			(this.channel.listen('onError_sendLLMMessage') satisfies Event<EventLLMMessageOnErrorParams>)((e) => {
				this.llmMessageHooks.onError[e.requestId]?.(e);
				this._clearChannelHooks(e.requestId);
				// Mask secrets in error logs
				const config = this.secretDetectionService.getConfig();
				if (config.enabled) {
					const redacted = this.secretDetectionService.redactSecretsInObject(e);
					console.error('Error in LLMMessageService:', JSON.stringify(redacted.redacted));
				} else {
					console.error('Error in LLMMessageService:', JSON.stringify(e));
				}
			})
		);
		// .list()
		this._register(
			(
				this.channel.listen('onSuccess_list_ollama') satisfies Event<EventModelListOnSuccessParams<OllamaModelResponse>>
			)((e) => {
				this.listHooks.ollama.success[e.requestId]?.(e);
			})
		);
		this._register(
			(this.channel.listen('onError_list_ollama') satisfies Event<EventModelListOnErrorParams<OllamaModelResponse>>)(
				(e) => {
					this.listHooks.ollama.error[e.requestId]?.(e);
				}
			)
		);
		this._register(
			(
				this.channel.listen('onSuccess_list_openAICompatible') satisfies Event<
					EventModelListOnSuccessParams<OpenaiCompatibleModelResponse>
				>
			)((e) => {
				this.listHooks.openAICompat.success[e.requestId]?.(e);
			})
		);
		this._register(
			(
				this.channel.listen('onError_list_openAICompatible') satisfies Event<
					EventModelListOnErrorParams<OpenaiCompatibleModelResponse>
				>
			)((e) => {
				this.listHooks.openAICompat.error[e.requestId]?.(e);
			})
		);
	}

	sendLLMMessage(params: ServiceSendLLMMessageParams) {
		const { onText, onFinalMessage, onError, onAbort, modelSelection, ...proxyParams } = params;

		// throw an error if no model/provider selected (this should usually never be reached, the UI should check this first, but might happen in cases like Apply where we haven't built much UI/checks yet, good practice to have check logic on backend)
		if (modelSelection === null) {
			const message = `Please add a provider in GRID Settings.`;
			onError({ message, fullError: null });
			return null;
		}

		if (params.messagesType === 'chatMessages' && (params.messages?.length ?? 0) === 0) {
			const message = `No messages detected.`;
			onError({ message, fullError: null });
			return null;
		}

		// Detect and redact secrets before sending
		const config = this.secretDetectionService.getConfig();
		if (config.enabled && params.messagesType === 'chatMessages' && params.messages) {
			let totalMatches: unknown[] = [];
			let hasAnySecrets = false;

			// Scan all messages for secrets
			for (const msg of params.messages) {
				// Handle different message types
				if ('content' in msg) {
					// AnthropicLLMChatMessage or OpenAILLMChatMessage
					if (typeof msg.content === 'string') {
						const detection = this.secretDetectionService.detectSecrets(msg.content);
						if (detection.hasSecrets) {
							hasAnySecrets = true;
							totalMatches.push(...detection.matches);
							// Redact the message content
							(msg as any).content = detection.redactedText;
						}
					} else if (Array.isArray(msg.content)) {
						// Handle array content (e.g., OpenAI format with images)
						for (const part of msg.content) {
							if ('type' in part && part.type === 'text' && 'text' in part && typeof part.text === 'string') {
								const detection = this.secretDetectionService.detectSecrets(part.text);
								if (detection.hasSecrets) {
									hasAnySecrets = true;
									totalMatches.push(...detection.matches);
									(part as any).text = detection.redactedText;
								}
							}
						}
					}
				} else if ('parts' in msg) {
					// GeminiLLMChatMessage - uses 'parts' instead of 'content'
					for (const part of msg.parts) {
						if ('text' in part && typeof part.text === 'string') {
							const detection = this.secretDetectionService.detectSecrets(part.text);
							if (detection.hasSecrets) {
								hasAnySecrets = true;
								totalMatches.push(...detection.matches);
								(part as any).text = detection.redactedText;
							}
						}
					}
				}
			}

			// Show warning if secrets detected
			if (hasAnySecrets) {
				const countByType = new Map<string, number>();
				for (const match of totalMatches) {
					const name = match.pattern.name;
					countByType.set(name, (countByType.get(name) || 0) + 1);
				}

				const typesList = Array.from(countByType.entries())
					.map(([name, count]) => `${name} (${count})`)
					.join(', ');

				if (config.mode === 'block') {
					// Always show block notifications (they're important)
					this.notificationService.warn(
						`Secret detected: ${typesList}. Message blocked from sending. Use environment variables or secure vaults instead of pasting keys into chat.`
					);
					onError({
						message: `Message blocked: Secrets detected (${typesList}). Please remove secrets before sending.`,
						fullError: null,
					});
					return null;
				} else {
					// Redact mode - silently redact without notification
					// (Notification removed per user request)
				}
			}
		}

		const { settingsOfProvider } = this.gridSettingsService.state;

		const mcpTools = this.mcpService.getMCPTools();

		// add state for request id
		const requestId = generateUuid();
		this.llmMessageHooks.onText[requestId] = onText;
		this.llmMessageHooks.onFinalMessage[requestId] = onFinalMessage;
		this.llmMessageHooks.onError[requestId] = onError;
		this.llmMessageHooks.onAbort[requestId] = onAbort; // used internally only

		// params will be stripped of all its functions over the IPC channel
		this.channel.call('sendLLMMessage', {
			...proxyParams,
			requestId,
			settingsOfProvider,
			modelSelection,
			mcpTools,
		} satisfies MainSendLLMMessageParams);

		return requestId;
	}

	abort(requestId: string) {
		this.llmMessageHooks.onAbort[requestId]?.(); // calling the abort hook here is instant (doesn't go over a channel)
		this.channel.call('abort', { requestId } satisfies MainLLMMessageAbortParams);
		this._clearChannelHooks(requestId);
	}

	ollamaList = (params: ServiceModelListParams<OllamaModelResponse>) => {
		const { onSuccess, onError, ...proxyParams } = params;

		const { settingsOfProvider } = this.gridSettingsService.state;

		// add state for request id
		const requestId_ = generateUuid();
		this.listHooks.ollama.success[requestId_] = onSuccess;
		this.listHooks.ollama.error[requestId_] = onError;

		this.channel.call('ollamaList', {
			...proxyParams,
			settingsOfProvider,
			providerName: 'ollama',
			requestId: requestId_,
		} satisfies MainModelListParams<OllamaModelResponse>);
	};

	openAICompatibleList = (params: ServiceModelListParams<OpenaiCompatibleModelResponse>) => {
		const { onSuccess, onError, ...proxyParams } = params;

		const { settingsOfProvider } = this.gridSettingsService.state;

		// add state for request id
		const requestId_ = generateUuid();
		this.listHooks.openAICompat.success[requestId_] = onSuccess;
		this.listHooks.openAICompat.error[requestId_] = onError;

		this.channel.call('openAICompatibleList', {
			...proxyParams,
			settingsOfProvider,
			requestId: requestId_,
		} satisfies MainModelListParams<OpenaiCompatibleModelResponse>);
	};

	private _clearChannelHooks(requestId: string) {
		delete this.llmMessageHooks.onText[requestId];
		delete this.llmMessageHooks.onFinalMessage[requestId];
		delete this.llmMessageHooks.onError[requestId];

		delete this.listHooks.ollama.success[requestId];
		delete this.listHooks.ollama.error[requestId];

		delete this.listHooks.openAICompat.success[requestId];
		delete this.listHooks.openAICompat.error[requestId];
	}
}

registerSingleton(ILLMMessageService, LLMMessageService, InstantiationType.Eager);
